{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b42b477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94cfce2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\keith\\Desktop\\repos\\whisper-streaming\\private-whisper\\whisper\\__init__.py:151: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "whisper_tiny = whisper.load_model(\"tiny.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0a5ef4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelDimensions(n_mels=80, n_audio_ctx=1500, n_audio_state=384, n_audio_head=6, n_audio_layer=4, n_vocab=51864, n_text_ctx=448, n_text_state=384, n_text_head=6, n_text_layer=4)\n"
     ]
    }
   ],
   "source": [
    "print(whisper_tiny.dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8839d040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisper.streaming import StreamingConfig\n",
    "\n",
    "model_dims = whisper_tiny.dims\n",
    "streaming_config = StreamingConfig()\n",
    "model = whisper.WhisperStreaming(model_dims, streaming_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8808b4c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['decoder.blocks.0.p_choose_layer.energy_bias', 'decoder.blocks.0.p_choose_layer.q_energy_proj.layers.0.weight', 'decoder.blocks.0.p_choose_layer.q_energy_proj.layers.0.bias', 'decoder.blocks.0.p_choose_layer.q_energy_proj.layers.2.weight', 'decoder.blocks.0.p_choose_layer.q_energy_proj.layers.2.bias', 'decoder.blocks.0.p_choose_layer.q_energy_proj.layers.4.weight', 'decoder.blocks.0.p_choose_layer.q_energy_proj.layers.4.bias', 'decoder.blocks.0.p_choose_layer.q_energy_proj.layers.6.weight', 'decoder.blocks.0.p_choose_layer.q_energy_proj.layers.6.bias', 'decoder.blocks.0.p_choose_layer.k_energy_proj.layers.0.weight', 'decoder.blocks.0.p_choose_layer.k_energy_proj.layers.0.bias', 'decoder.blocks.0.p_choose_layer.k_energy_proj.layers.2.weight', 'decoder.blocks.0.p_choose_layer.k_energy_proj.layers.2.bias', 'decoder.blocks.0.p_choose_layer.k_energy_proj.layers.4.weight', 'decoder.blocks.0.p_choose_layer.k_energy_proj.layers.4.bias', 'decoder.blocks.0.p_choose_layer.k_energy_proj.layers.6.weight', 'decoder.blocks.0.p_choose_layer.k_energy_proj.layers.6.bias', 'decoder.blocks.1.p_choose_layer.energy_bias', 'decoder.blocks.1.p_choose_layer.q_energy_proj.layers.0.weight', 'decoder.blocks.1.p_choose_layer.q_energy_proj.layers.0.bias', 'decoder.blocks.1.p_choose_layer.q_energy_proj.layers.2.weight', 'decoder.blocks.1.p_choose_layer.q_energy_proj.layers.2.bias', 'decoder.blocks.1.p_choose_layer.q_energy_proj.layers.4.weight', 'decoder.blocks.1.p_choose_layer.q_energy_proj.layers.4.bias', 'decoder.blocks.1.p_choose_layer.q_energy_proj.layers.6.weight', 'decoder.blocks.1.p_choose_layer.q_energy_proj.layers.6.bias', 'decoder.blocks.1.p_choose_layer.k_energy_proj.layers.0.weight', 'decoder.blocks.1.p_choose_layer.k_energy_proj.layers.0.bias', 'decoder.blocks.1.p_choose_layer.k_energy_proj.layers.2.weight', 'decoder.blocks.1.p_choose_layer.k_energy_proj.layers.2.bias', 'decoder.blocks.1.p_choose_layer.k_energy_proj.layers.4.weight', 'decoder.blocks.1.p_choose_layer.k_energy_proj.layers.4.bias', 'decoder.blocks.1.p_choose_layer.k_energy_proj.layers.6.weight', 'decoder.blocks.1.p_choose_layer.k_energy_proj.layers.6.bias', 'decoder.blocks.2.p_choose_layer.energy_bias', 'decoder.blocks.2.p_choose_layer.q_energy_proj.layers.0.weight', 'decoder.blocks.2.p_choose_layer.q_energy_proj.layers.0.bias', 'decoder.blocks.2.p_choose_layer.q_energy_proj.layers.2.weight', 'decoder.blocks.2.p_choose_layer.q_energy_proj.layers.2.bias', 'decoder.blocks.2.p_choose_layer.q_energy_proj.layers.4.weight', 'decoder.blocks.2.p_choose_layer.q_energy_proj.layers.4.bias', 'decoder.blocks.2.p_choose_layer.q_energy_proj.layers.6.weight', 'decoder.blocks.2.p_choose_layer.q_energy_proj.layers.6.bias', 'decoder.blocks.2.p_choose_layer.k_energy_proj.layers.0.weight', 'decoder.blocks.2.p_choose_layer.k_energy_proj.layers.0.bias', 'decoder.blocks.2.p_choose_layer.k_energy_proj.layers.2.weight', 'decoder.blocks.2.p_choose_layer.k_energy_proj.layers.2.bias', 'decoder.blocks.2.p_choose_layer.k_energy_proj.layers.4.weight', 'decoder.blocks.2.p_choose_layer.k_energy_proj.layers.4.bias', 'decoder.blocks.2.p_choose_layer.k_energy_proj.layers.6.weight', 'decoder.blocks.2.p_choose_layer.k_energy_proj.layers.6.bias', 'decoder.blocks.3.p_choose_layer.energy_bias', 'decoder.blocks.3.p_choose_layer.q_energy_proj.layers.0.weight', 'decoder.blocks.3.p_choose_layer.q_energy_proj.layers.0.bias', 'decoder.blocks.3.p_choose_layer.q_energy_proj.layers.2.weight', 'decoder.blocks.3.p_choose_layer.q_energy_proj.layers.2.bias', 'decoder.blocks.3.p_choose_layer.q_energy_proj.layers.4.weight', 'decoder.blocks.3.p_choose_layer.q_energy_proj.layers.4.bias', 'decoder.blocks.3.p_choose_layer.q_energy_proj.layers.6.weight', 'decoder.blocks.3.p_choose_layer.q_energy_proj.layers.6.bias', 'decoder.blocks.3.p_choose_layer.k_energy_proj.layers.0.weight', 'decoder.blocks.3.p_choose_layer.k_energy_proj.layers.0.bias', 'decoder.blocks.3.p_choose_layer.k_energy_proj.layers.2.weight', 'decoder.blocks.3.p_choose_layer.k_energy_proj.layers.2.bias', 'decoder.blocks.3.p_choose_layer.k_energy_proj.layers.4.weight', 'decoder.blocks.3.p_choose_layer.k_energy_proj.layers.4.bias', 'decoder.blocks.3.p_choose_layer.k_energy_proj.layers.6.weight', 'decoder.blocks.3.p_choose_layer.k_energy_proj.layers.6.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(whisper_tiny.state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80c3efb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=384, out_features=384, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.decoder.blocks[0].p_choose_layer.q_energy_proj.layers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89e93625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stream source yield\n",
      "stream source yield\n",
      "log mel yield\n",
      "stream source yield\n",
      "log mel yield\n",
      "encoder len torch.Size([80, 200])\n",
      "stream source yield\n",
      "log mel yield\n",
      "encoder len torch.Size([80, 400])\n",
      "READ\n",
      "Remove Hooks\n",
      "<|startoftranscript|>\n",
      "Install hooks\n",
      "break read 0.07628928124904633 False\n",
      "stream source yield\n",
      "log mel yield\n",
      "encoder len torch.Size([80, 600])\n",
      "READ\n",
      "Remove Hooks\n",
      "<|startoftranscript|>\n",
      "Install hooks\n",
      "break read 0.07631254196166992 False\n",
      "stream source yield\n",
      "log mel yield\n",
      "encoder len torch.Size([80, 800])\n",
      "READ\n",
      "Remove Hooks\n",
      "<|startoftranscript|>\n",
      "Install hooks\n",
      "break read 0.07631116360425949 False\n",
      "log mel yield\n",
      "encoder len torch.Size([80, 1000])\n",
      "READ\n",
      "Remove Hooks\n",
      "<|startoftranscript|>\n",
      "Install hooks\n",
      "break read 0.07633952051401138 False\n",
      "encoder len torch.Size([80, 1100])\n",
      "READ\n",
      "Remove Hooks\n",
      "<|startoftranscript|>\n",
      "Install hooks\n",
      "break read 0.07631587237119675 False\n",
      "READ\n",
      "Remove Hooks\n",
      "<|startoftranscript|>\n",
      "Install hooks\n",
      "<|startoftranscript|>\n",
      "<|startoftranscript|> And\n",
      "<|startoftranscript|> And so\n",
      "<|startoftranscript|> And so my\n",
      "<|startoftranscript|> And so my fellow\n",
      "<|startoftranscript|> And so my fellow Americans\n",
      "<|startoftranscript|> And so my fellow Americans ask\n",
      "<|startoftranscript|> And so my fellow Americans ask not\n",
      "<|startoftranscript|> And so my fellow Americans ask not what\n",
      "<|startoftranscript|> And so my fellow Americans ask not what your\n",
      "<|startoftranscript|> And so my fellow Americans ask not what your country\n",
      "<|startoftranscript|> And so my fellow Americans ask not what your country can\n",
      "<|startoftranscript|> And so my fellow Americans ask not what your country can do\n",
      "<|startoftranscript|> And so my fellow Americans ask not what your country can do for\n",
      "<|startoftranscript|> And so my fellow Americans ask not what your country can do for you\n",
      "<|startoftranscript|> And so my fellow Americans ask not what your country can do for you\n",
      "<|startoftranscript|> And so my fellow Americans ask not what your country can do for you\n",
      "<|startoftranscript|> And so my fellow Americans ask not what your country can do for you ask\n",
      "<|startoftranscript|> And so my fellow Americans ask not what your country can do for you ask what\n",
      "<|startoftranscript|> And so my fellow Americans ask not what your country can do for you ask what you\n",
      "<|startoftranscript|> And so my fellow Americans ask not what your country can do for you ask what you can\n",
      "<|startoftranscript|> And so my fellow Americans ask not what your country can do for you ask what you can do\n",
      "<|startoftranscript|> And so my fellow Americans ask not what your country can do for you ask what you can do for\n",
      "<|startoftranscript|> And so my fellow Americans ask not what your country can do for you ask what you can do for your\n",
      "<|startoftranscript|> And so my fellow Americans ask not what your country can do for you ask what you can do for your country\n",
      "<|startoftranscript|> And so my fellow Americans ask not what your country can do for you ask what you can do for your country\n",
      "break finished False tensor(True)\n",
      "WRITE  And so my fellow Americans ask not what your country can do for you ask what you can do for your country\n",
      "Remove Hooks\n",
      " And so my fellow Americans ask not what your country can do for you ask what you can do for your country\n"
     ]
    }
   ],
   "source": [
    "print(model.transcribe_stream(\"./jfk.wav\", chunk_size=32000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abe1a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 80, 3000])\n",
      "torch.Size([1, 1500, 384])\n"
     ]
    }
   ],
   "source": [
    "audio = whisper.load_audio(file=\"jfk.wav\", sr=16000)\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "mel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels)\n",
    "\n",
    "mel = mel.unsqueeze(0)\n",
    "xa = model.encoder(mel)\n",
    "\n",
    "print(mel.shape)\n",
    "print(xa.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bf91508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def monotonic_alignment(p):  # p: [B, T, S]\n",
    "    bsz, tgt_len, src_len = p.size()\n",
    "    \n",
    "    # Extension probability matrix: roll, unsqueeze, expand, upper-triangular\n",
    "    p_ext = p.roll(1, [-1]).unsqueeze(-2).expand(-1, -1, src_len, -1).triu(1)\n",
    "    \n",
    "    # Transition matrix: 1 - p_ext, then cumulative product, upper-triangular\n",
    "    T = (1 - p_ext).cumprod(-1).triu()\n",
    "    \n",
    "    # Base case for alpha: alpha_0 = p_0 * T_0\n",
    "    alpha = [p[:, [0]] * T[:, [0]]]\n",
    "    \n",
    "    # Recurrence over target steps\n",
    "    for i in range(1, tgt_len):\n",
    "        print(alpha[i - 1].size())\n",
    "        print(T[:, i].size())\n",
    "        alpha.append(p[:, [i]] * torch.bmm(alpha[i - 1], T[:, i]))\n",
    "\n",
    "    return torch.cat(alpha[1:], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "982762c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6, 1, 750])\n",
      "torch.Size([4, 6, 2, 750])\n",
      "torch.Size([4, 6, 3, 750])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[99], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(p_choose\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time_step \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 27\u001b[0m     alpha \u001b[38;5;241m=\u001b[39m \u001b[43mmonotonic_alignment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_choose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(alpha)\n\u001b[0;32m     30\u001b[0m next_token \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[97], line 4\u001b[0m, in \u001b[0;36mmonotonic_alignment\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmonotonic_alignment\u001b[39m(p):  \u001b[38;5;66;03m# p: [B, T, S]\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     bsz, tgt_len, src_len \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Extension probability matrix: roll, unsqueeze, expand, upper-triangular\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     p_ext \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mroll(\u001b[38;5;241m1\u001b[39m, [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, src_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtriu(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "from whisper.tokenizer import get_tokenizer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# load audio and encode\n",
    "audio = whisper.load_audio(\"jfk.wav\", sr=16000)\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "mel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels)\n",
    "\n",
    "with torch.no_grad():\n",
    "    xa = model.encoder(mel.unsqueeze(0))  # (1, n_audio_ctx, n_audio_state)\n",
    "\n",
    "# initialize tokens\n",
    "tokenizer = get_tokenizer(multilingual=False)\n",
    "tokens = torch.tensor([tokenizer.sot_sequence], device=device)  # (1, T)\n",
    "\n",
    "for time_step in range(100):  # max steps\n",
    "    logits, p_choose = model.decoder.decode_with_pchoose(tokens, xa)\n",
    "    \n",
    "    _, tgt_len, src_len = p_choose.size()\n",
    "\n",
    "    p_choose = p_choose.view(model.dims.n_text_layer, -1, tgt_len, src_len)\n",
    "    print(p_choose.size())\n",
    "    \n",
    "    if time_step > 1:\n",
    "        alpha = monotonic_alignment(p_choose)\n",
    "        print(alpha)\n",
    "\n",
    "    next_token = logits[:, -1].argmax(dim=-1, keepdim=True)\n",
    "    # print(tokenizer.decode(next_token[0].tolist()))\n",
    "\n",
    "    tokens = torch.cat([tokens, next_token], dim=-1)\n",
    "    \n",
    "    if time_step == 3:\n",
    "        break\n",
    "\n",
    "    if next_token.item() == tokenizer.eot:\n",
    "        break\n",
    "\n",
    "# decode final output\n",
    "print(len(model.decoder.blocks))\n",
    "print(\"Decoded text:\", tokenizer.decode(tokens[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "54bb7b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50258, 50259, 50359, 50257]])\n"
     ]
    }
   ],
   "source": [
    "tokens = torch.tensor([tokenizer.sot_sequence]) \n",
    "\n",
    "for _ in range(100):\n",
    "    logits, _ = model.decoder.decode_with_pchoose(tokens, xa)\n",
    "    next_token = logits[:, -1].argmax(dim=-1, keepdim=True)  # greedy decode\n",
    "    tokens = torch.cat([tokens, next_token], dim=-1)\n",
    "    print(tokens)\n",
    "\n",
    "    if next_token.item() == tokenizer.eot:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "84972b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|startoftranscript|><|en|><|transcribe|><|endoftext|>'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokens.squeeze(0)\n",
    "tokenizer.decode(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmchat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
